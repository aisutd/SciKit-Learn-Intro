{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SciKit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Amol Mavuduru\n",
    "\n",
    "Notebook Created: October 2017\n",
    "\n",
    "This notebook is designed to give you a brief introduction to scikit-learn, a library that has become one of the most popular machine learning frameworks for python. This library is designed with Object-Oriented Programming (OOP) principles in mind and consequently it is intuitive and easy to use. In just a few lines of code, you can initialize, train, and test anything from simple algorithms like Linear Classifiers and Decision Trees to cutting-edge machine learning algorithms such as Support Vector Machines, Random Forests, and even deep Neural Networks. \n",
    "\n",
    "The documentation website for scikit-learn can be found here: http://scikit-learn.org/stable/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the following link: http://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the following dependencies:\n",
    "    - Python (>= 2.7 or >= 3.3) (If you have 2.7 or 3.3 or higher either should be fine)\n",
    "    - Numpy\n",
    "    - Pandas (This is specific to our demo)\n",
    "    - Scipy \n",
    "\n",
    "If you don't have Scipy, but you have Python and Numpy you should be fine, since if pip does not find Scipy on your system it will probably install it for you when you install scikit-learn.\n",
    "\n",
    "To install sklearn, just go to your terminal and issue the following command:\n",
    "##### pip install sklearn\n",
    "\n",
    "Note that you can install all of the above dependencies using pip as well.\n",
    "\n",
    "Once scikit-learn is installed you should be ready to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Demo: Working with an Actual Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we are going to use scikit-learn to train and test machine learning models on a real dataset! This dataset is designed to aid in Breast Cancer diagnosis and has been made publicly available at both the UCI Machine Learning repository, and on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Name: Breast Cancer Wisconsin (Diagnostic)\n",
    "- Kaggle Link: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "- UCI Machine Learning Link: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np   # For linear algebra\n",
    "import pandas as pd  # For data preprocessing and CSV File I/O\n",
    "import sklearn  # For training machine learning models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries below are optional, but I will use them for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in and exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data.csv file should be in the same directory as this notebook and this is the case in the Github repository. We can use pandas to read in the csv file as an object called a dataframe using the read_csv function.\n",
    "\n",
    "After that we can display the first 5 rows of the dataframe using the head() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains several features and a diagnosis label (or class) that is either B or M to indicate a benign or malignant tumor respectively.\n",
    "Now that we know what our data looks like, we can get some general information about the data using the info() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "Unnamed: 32                0 non-null float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at some general statistics about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean     ...       texture_worst  perimeter_worst  \\\n",
       "count     569.000000     ...          569.000000       569.000000   \n",
       "mean        0.181162     ...           25.677223       107.261213   \n",
       "std         0.027414     ...            6.146258        33.602542   \n",
       "min         0.106000     ...           12.020000        50.410000   \n",
       "25%         0.161900     ...           21.080000        84.110000   \n",
       "50%         0.179200     ...           25.410000        97.660000   \n",
       "75%         0.195700     ...           29.720000       125.400000   \n",
       "max         0.304000     ...           49.540000       251.200000   \n",
       "\n",
       "        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "       Unnamed: 32  \n",
       "count          0.0  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to do a little bit of preprocessing and split our data into a set of features (X) and a set of target labels (y). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 32', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop(['id', 'diagnosis'], axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean           ...             radius_worst  \\\n",
       "0                 0.07871           ...                    25.38   \n",
       "1                 0.05667           ...                    24.99   \n",
       "2                 0.05999           ...                    23.57   \n",
       "3                 0.09744           ...                    14.91   \n",
       "4                 0.05883           ...                    22.54   \n",
       "\n",
       "   texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0          17.33           184.60      2019.0            0.1622   \n",
       "1          23.41           158.80      1956.0            0.1238   \n",
       "2          25.53           152.50      1709.0            0.1444   \n",
       "3          26.50            98.87       567.7            0.2098   \n",
       "4          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    M\n",
       "1    M\n",
       "2    M\n",
       "3    M\n",
       "4    M\n",
       "Name: diagnosis, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we always need a way to evaluate our models. In order to do this, we want to split our data into a training set and a testing set. Rather than training our model on the entire dataset for the purpose evaluation, we only train the model on the training test first and then evaluate the model's performance in predicting the class of samples on the test set. \n",
    "\n",
    "This practice, which is sometimes referred to as cross-validation, is designed to simulate our model's performance on unseen data and provides a measure of the ability of our model to generalize to a broader variety of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # This function is necessary to split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What train_test_split does:\n",
    "\n",
    "- Takes in two dataframes, X and y as parameters corresponding to the features and target column. \n",
    "- Contains a parameter called test_size that allows you to specify the proportion of the data that is used for the test set. In the case we will make the test set 30% of our data, which is usually pretty standard.\n",
    "- The random_state parameter allows us to specify the random seed so that the results are reproducible.\n",
    "- The function returns a tuple of four objects:\n",
    "    - X_train: Dataframe with feature values for training set.\n",
    "    - X_test: Dataframe with feature values for testing set.\n",
    "    - y_train: Dataframe with labels (diagnosis) for training samples.\n",
    "    - y_test: Dataframe with labels (diagnosis) for testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Machine Learning Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data into training and test sets, we are ready to train a machine learning model using scikit-learn! We will be using a Logistic Regression model that unlike the name, is actually used for classification tasks such as this one. The Logistic Regression algorithm relies on a function called the sigmoid function, which has the graph of an S-shaped curve and maps any real value to a value in the range (0, 1).\n",
    "\n",
    "The equation for the function is f(z) = 1/(1 + e^-z) and I have plotted this function below for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111a8f6a0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH1ZJREFUeJzt3XmUFOW9//F39TLTzAwgJAOuJwGVR36JG0uiETWacJHF\nxIvi/YWIEUElMZvLTUzillyTG3OCCokLKOq9kuWXIYLIIMIRNQHEJeiVRH0MAoerCTjIOCyz9HR3\n/f6oHmjGWXpmurt6+bzOqdPdVd09X4rqzzzz9FP1OK7rIiIihSvgdwEiItI3CnIRkQKnIBcRKXAK\nchGRAqcgFxEpcKFc/8C6un29HiYzaFAF9fWNmSwnI/K1Lsjf2lRXz6iuninGuqqr+zudbSuoFnko\nFPS7hA7la12Qv7Wprp5RXT1TanUVVJCLiMhHKchFRAqcglxEpMApyEVECpyCXESkwCnIRUQKXFpB\nboz5rDHmuQ7WX2iMedkY84Ix5qqMVyciIt3q9oQgY8z3gBnAgXbrw8DdwNjktvXGmOXW2l3ZKFRE\nCls0Co2N0NTk0NwMLS0O0Si0tBy6H41Ca6tDLMbBpe1xIuE9jschHndIJLz7iYS3uO6h+5EI7N9f\nfnB96tLRuvZX805dd+jWafe4439n6vrU+4EAXHcdDB+egZ3ZTjpndr4DTAUea7d+JLDFWlsPYIxZ\nB5wD1HT1ZoMGVfRpUHx1df9evzab8rUuyN/aVFfP5ENd8Tjs3g3//Ke37NwJ77/fn/p6+PBDDrtt\naID9++HAAW+JxXJdbVmuf2C3RoyAn/wk8/+P3Qa5tfaPxphPdrBpANCQ8ngfMLC79+vLabPV1f2p\nq9vX69dnS77WBflbm+rqmVzW1dAAW7cGDi7btnnLu+867N7tEI93eqb4QWVlLgMGuFRWwtFHu1RU\nQEWFS2Wld7+8HMrLXcrLved6jyEcdgmHIRTylnDYPXg/GGxb3IP3A4FDt4EAOA4EAi6DB1eyd++B\n5GNvPXDY448u7sHnpN52tK6j56Tq6LWBAJxySlWv/x+7+kXel2ut7AVS37k/8GEf3k9EcqyhAV59\nNcimTcHkbYC6uo9+dRYOuxxzjMuYMXGGDHEZOtRbjj++nEikkYEDXQYOJHnrEon48I9JUV0NdXUJ\nf4voQGfB31d9CfI3gRONMYOB/XjdKr/MSFUikhX798Ozz4ZYsybEK68E2LLl8G7OY45JMH58jGHD\nEgwblmD4cG859livFdxedXU5dXXxHFUvnelxkBtjpgNV1tqFxpjrgafxRr88bK19L9MFikjffPCB\nw+rVQVauDPP880Gam71mYf/+LmefHWP06Dinn55g1Kg4Q4dqDt9ClFaQW2u3A2ck7/82Zf2TwJNZ\nqUxEei0Wg+XLQzz2WJgXXgiSSHjhPXJknIkTY0ycGOPkkxMEdCZJUcj59chFJHsaG+F3vwtz//1l\n7NjhpfSYMXEmTWpl0qQYw4erxV2MFOQiReCDDxwefjjMokVh9uwJEIm4zJwZZc6cKMOGKbyLnYJc\npIC1tMDdd5fxwANlNDY6HHGEy/XXtzBrVivV1QrwUqEgFylQr78e4FvfivDmm0GOOirBD3/YwvTp\nrVRV+V2Z5JqCXKTARKNw111lzJtXRjzu8LWvRbntthYFeAlTkIsUkM2bvVb4G28EOfbYBHff3cS5\n52ocd6nT4CORAuC6MH9+GRMmVPDGG0FmzIjy/PMHFOICqEUukvdc17tq3rx55Rx9tNcKP+88Bbgc\noiAXyWPxOPz7v5ezeDGcdFKcmpomnX0pH6EgF8lTra3wrW9FePzxMKNHw29+08jgwX5XJflIQS6S\nh1pa4OqrIzz1VJixY+OsWRMkGvW7KslX+rJTJM80NsLll/fjqafCnH12jD/8oZGB3V7pX0qZWuQi\neaS5GaZP78eGDSHGj4+xaFGT79f2lvynFrlIHrnttnI2bAgxZUorjzyiEJf0KMhF8sSyZSEeeaSM\nkSPj3HtvM2X5N+Wk5CkFuUgeeOcdh+uui1BZ6bJoURP9+vldkRQS9ZGL+KypCWbN6seBAw4PPNDE\nCSdonLj0jFrkIj67+eZy3ngjyOWXR5k6NeZ3OVKAFOQiPlqyJMRjj5Xx6U/HueOOFr/LkQKlIBfx\nydtvB7jxxghVVS4PPaQRKtJ76iMX8UFjI8yeHaGx0eGhh5o0l6b0iVrkIj6YP7+Mt94KcuWVUb70\nJfWLS98oyEVy7L33HO6/v4yhQxPccov6xaXv1LUikmM/+1k5TU0Od97ZTGWl39VIMVCLXCSHXnst\nQE1NmJNPjnPppepSkcxQkIvkiOvCrbeWA/DjH7cQ0KdPMkSHkkiO1NaG2LgxxAUXtDJunKZqk8xR\nkIvkQDQKP/lJOaGQy6236gtOySwFuUgOPPxwmO3bA8yc2aprqUjGKchFsmzPHpg7t5yBA11uuEGt\ncck8BblIls2dW05Dg8MNN7Ro8mTJim7HkRtjAsB9wKlACzDbWrslZftXgRuAOPCwtfb+LNUqUnC2\nbHF45JEww4YluPLKVr/LkSKVTov8IiBirT0TuAmY2277L4EvAmcBNxhjBmW2RJHCdddd5cRiDrfc\n0qIZfyRr0gnyccAqAGvtRmBMu+2vAwOBCOAA+iZHBNi502HZshAjRsSZPFkn/0j2pHOK/gCgIeVx\n3BgTsta2HZl/Bf4CHAAet9Z+2NWbDRpUQSgU7FWxANXV/Xv92mzK17ogf2sr9rrmzYNYDG64IciQ\nIX1/z2LfX5lWSnWlE+R7gdSfHGgLcWPMKcBkYBiwH1hsjJlmra3p7M3q6xt7XWx1dX/q6vb1+vXZ\nkq91Qf7WVux1NTXB/fdXMngwXHDBAerq8qOuTFNdPdOXurr6BZBO18p6YBKAMeYMYHPKtgagCWiy\n1saB9wH1kUvJW7IkzJ49AS6/vFUTKUvWpdMiXwqMN8ZswOsDn2mMmQ5UWWsXGmMWAOuMMVHgHeDR\nrFUrUgBcFxYuDBMKucycqZEqkn3dBrm1NgHMabf6rZTtDwAPZLgukYL1/PNBrA1y8cWtHHWUvvuX\n7NMJQSIZtmCBN87wmmuiPlcipUJBLpJBf/97gGeeCfGZz8Q47bSE3+VIiVCQi2TQgw+GAbjmGvWN\nS+4oyEUypL4e/vCHMMcdl2DiRJ0AJLmjIBfJkMceK6Ox0WHWrCghzYYrOaQgF8mA1lbvmuOVlS6X\nXaZuFcktBblIBtTWhvjHPwJ85SutDBjgdzVSahTkIhmwaFEYx3GZPVtDDiX3FOQifbR9u8OLL4YY\nNy7O8OE6AUhyT0Eu0kdLlnhDDqdNU9+4+ENBLtIHrusFeb9+LlOmaMih+ENBLtIHmzYF2Lo1wMSJ\nMaqq/K5GSpWCXKQPamrUrSL+U5CL9FI0CsuWhaiuTnDuuXG/y5ESpiAX6aW1a4Ps2RNg6tSYzuQU\nXynIRXqprVvlkkvUrSL+UpCL9EJDA6xeHWLEiDinnKLL1Yq/FOQivbB8eZiWFodp02I4jt/VSKlT\nkIv0wpIlXqf4xRerW0X8pyAX6aEdOxxeeCHEWWfFOPZYnZIv/lOQi/TQH/+oseOSXxTkIj3gulBT\nEyIS0Sn5kj8U5CI98NprAbZsCTJhQkzXHZe8oSAX6QGdki/5SEEukqZYzDsl/+MfT3DeeTolX/KH\nglwkTS++GGT37gCTJ8cIh/2uRuQQBblImlas8MaO60tOyTcKcpE0JBKwcmWII45w+dzn1K0i+UVB\nLpKGTZsC/POfASZMULeK5B8FuUgaamu99J4yRaNVJP8oyEW64bpQWxuistLVBBKSlxTkIt34298C\nbN8eYPz4GJGI39WIfFS385oYYwLAfcCpQAsw21q7JWX7WOAuwAF2ApdZa5uzU65I7tXWeh+TyZM1\nWkXyUzot8ouAiLX2TOAmYG7bBmOMAzwIzLTWjgNWAZ/IRqEifqmtDVFe7vKFLyjIJT85rtv1ZTiN\nMXcBL1lrf598/J619pjkfYPXWn8L+DRQa639RVfvF4vF3VAomInaRbLOWjjpJPjSl+CJJ/yuRkpc\np1OYpDNl7ACgIeVx3BgTstbGgI8DnwO+CWwBVhhjXrHWru3szerrG9MruQPV1f2pq9vX69dnS77W\nBflbW6HUtXhxGVDOF7/YRF2dfy3yQtlf+aIY66qu7t/ptnS6VvYCqe8QSIY4wAfAFmvtm9baVryu\nlTG9qlIkD61YESIUcpkwQd0qkr/SCfL1wCQAY8wZwOaUbVuBKmPMCcnHZwN/y2iFIj55912H114L\nctZZcQYN8rsakc6l07WyFBhvjNmA10cz0xgzHaiy1i40xswCfpv84nODtbY2i/WK5EzbaBVdW0Xy\nXbdBbq1NAHParX4rZfta4DMZrkvEd7W1IRzH5YILFOSS33RCkEgHdu1yePHFIJ/9bJyhQzXBsuQ3\nBblIB1atCuG6jk4CkoKgIBfpQFv/+KRJCnLJfwpykXY+/BDWrQty2mlxjjtO3SqS/xTkIu2sWRMi\nFnPUGpeCoSAXaWfVKq9bZeJEBbkUBgW5SIqWFli7NsSwYQlGjEj4XY5IWhTkIinWroUDBxwuuCCG\n0+klikTyi4JcJEXbFQ7VrSKFREEukpRIeEH+sY8lGDtWU7pJ4VCQiyS9+mqAnTth/Pg4QV0yXwqI\nglwkqW20iq6tIoVGQS6StGpViEgEzj1XQS6FRUEuAmzd6mBtkPHjobLS72pEekZBLsKhbpUvf9nn\nQkR6QUEughfkjuNy4YV+VyLScwpyKXkffODw0ktBxo6NM2SI39WI9JyCXEremjVBEglHo1WkYCnI\npeQ99ZQukiWFTUEuJa2xEZ57LsSJJ8Y5/nhde1wKk4JcStqf/hSkqUndKlLYFORS0nQ2pxQDBbmU\nrHgcVq8OUV2dYPRoXXtcCpeCXErWK68E2b07wIQJMQL6JEgB0+ErJWvlSo1WkeKgIJeS5LpQWxui\nqsrlnHN07XEpbApyKUl//WuAHTsC/Mu/xCgv97sakb5RkEtJqq31ulUmT1a3ihQ+BbmUpBUrQkQi\nLuefryCXwqcgl5Lz9tsB3n47yHnnxXTtcSkKCnIpOW2jVdStIsUi1N0TjDEB4D7gVKAFmG2t3dLB\n8xYCe6y1N2W8SpEMWrEiRCjkMmGCglyKQzot8ouAiLX2TOAmYG77JxhjrgFOznBtIhm3Y4fD668H\nOfvsOAMH+l2NSGakE+TjgFUA1tqNwJjUjcaYzwGfBRZkvDqRDGsbrTJlilrjUjy67VoBBgANKY/j\nxpiQtTZmjDkKuA34V+DSdH7goEEVhELBnleaVF3dv9evzaZ8rQvytzY/6lq9GgIBuOyyCNXVkQ6f\no/3VM6qrZ7JRVzpBvhdI/ckBa21bc2Ya8HFgJXAkUGGMecta+2hnb1Zf39jLUr0dUFe3r9evz5Z8\nrQvytzY/6tq1y2HDhkrOPDOO4zRRV5cfdaVDdfVMMdbV1S+AdIJ8PXAh8AdjzBnA5rYN1tr5wHwA\nY8wVwEldhbiIn1auDOG6jkarSNFJJ8iXAuONMRsAB5hpjJkOVFlrF2a1OpEMausfnzRJQS7Fpdsg\nt9YmgDntVr/VwfMezVBNIhm3Zw+sXx9k1Kg4xxyjKd2kuOiEICkJq1eHiMcdtcalKCnIpSSsWBEG\nYMqUVp8rEck8BbkUvf374bnngowcGWf4cHWrSPFRkEvRW7MmRDSq0SpSvBTkUvSeeEJnc0pxU5BL\nUauv91rkI0fGGTky4Xc5IlmhIJei9sQTYVpbHS65JIbj+F2NSHYoyKWo1dSEcRyXiy/WaBUpXgpy\nKVrbtzu8/HKQcePiHH20RqtI8VKQS9FassQbOz5tmlrjUtwU5FKUXNfrVunXz9VoFSl6CnIpSn/5\nS4Bt2wJMnBijqsrvakSyS0EuRammxutWufRSdatI8VOQS9GJRmHZsjDV1QnOOSfudzkiWacgl6Lz\nzDMh6usdpk6NEUrnivsiBU5BLkVnyRIvvTVaRUqFglyKSkODd+1xY+KcfLJOyZfSoCCXorJ8eZiW\nFodp03RKvpQOBbkUlZqakE7Jl5KjIJeisWOHw8aNIc46S/NySmlRkEvR0Cn5UqoU5FIU4nH4/e/D\nRCI6JV9Kj4JcisKaNUG2bw9wySWt9O/vdzUiuaUgl6KwcGEZAFddpW4VKT0Kcil4mzcHWLcuxLnn\nxjSdm5QkBbkUvAcf9Frj11wT9bkSEX8oyKWgvf++w+OPhzj++ATnn68LZElpUpBLQXv00TDRqMNV\nV0UJ6GiWEqVDXwpWc7MX5AMHuvzbv+lLTildCnIpWMuWhdi9O8CMGVEqK/2uRsQ/CnIpSK4LDzxQ\nRjDoMmuWWuNS2hTkUpDWrw/yxhtBLrwwpuuqSMnrdv4UY0wAuA84FWgBZltrt6Rs/wrwXSAGbAa+\nYa3VYF7JqoULveuqXH21hhyKpNMivwiIWGvPBG4C5rZtMMb0A+4AzrPWngUMBKZko1CRNlu3Ojz9\ndIjRo+OMGaM2g0g6MxqOA1YBWGs3GmPGpGxrAT5nrW1Meb/mrt5s0KAKQqFgb2oFoLo6Py+kka91\nQf7W1tu6/uM/vD7yG28MZuXfVmz7K9tUV89ko650gnwA0JDyOG6MCVlrY8kulF0AxphvAVXAmq7e\nrL6+savNXaqu7k9d3b5evz5b8rUuyN/aelvX++87LFpUydFHu5xzzgHq6vKjrmxTXT1TjHV19Qsg\nnSDfC6S+Q8Bae/A6ock+9F8AI4CLrbX65kmy5he/KKOx0eGWW1oIh/2uRiQ/pNNHvh6YBGCMOQPv\nC81UC4AIcFFKF4tIxr35ZoDFi8OMGBHna1/TkEORNum0yJcC440xGwAHmGmMmY7XjfIKMAv4M7DW\nGAMwz1q7NEv1Sgm7/fZyEgmH229vIZTOkStSIrr9OCT7wee0W/1Wyn2NRZesW7s2yLPPepeq/cIX\ndHEskVQKYcl7sRjcdls5juNy++0tOI7fFYnkFwW55L3Fi8NYG+SrX23lU5/SuHGR9hTkktf27fNG\nqlRUuHz/+zqLU6QjCnLJa/PmlbF7d4DvfCfK0KEa2SrSEQW55K0dOxwWLCjjmGMSzJmj1rhIZxTk\nkrd++tNyWlocfvSjFvr187sakfylIJe89NxzQZYuDXP66XGmTo11/wKREqYgl7yzc6fDN74RIRx2\nufPOZs3FKdINnR8neSUWg6uvjrB7d4Cf/rSZ007TcEOR7qitI3nlzjvL2LgxxJQprcyereupiKRD\nQS5545lngsybV84nPpHgnnuadQanSJoU5JIX3nvP4dprI5SVuSxa1MSAAX5XJFI41Ecuvmtthauv\n7seePQHuvLOZU05Rv7hIT6hFLr772c/KefnlIBdd1MoVV6hfXKSnFOTiq8cfD3HvvWUMH55g7lz1\ni4v0hoJcfPPb34b4+tcj9O/v8tBDTfTPz7lyRfKeglx88atfwXe/249Bg1yWLm3k059Wv7hIbynI\nJefmzy/j29+GIUMSLF3apC83RfpIo1YkZ1wXfv7zMu6+u5zjjoOamkaGD9elaUX6SkEuOeG6cOut\n5SxYUMawYQmefTZARYVCXCQT1LUiWffBBw6zZ0dYsKAMY+IsX97IJz7hd1UixUNBLllVWxvi7LMr\nePLJMGPHxlm2rEkz/YhkmIJcsmLPHpgzJ8LMmf3Yt8/httuaWb68kY99TCEukmnqI5eMW7UqyI03\nRnj//QCjRsWZP7+ZESM0MkUkWxTkkjGbNwe4554ynnwyTFmZy803t/CNb0QJ6SgTySp9xKRPXBf+\n/Ocgv/51Gc895x1Oo0bFueeeZk46Sa1wkVxQkEuvxGLeF5m//nUZ//M/QQDGjYvxzW9GOe+8uK6Z\nIpJDCnJJWzwOL70UZOXKELW1Id59N4DjuEyZ0so3vxll1Ci1wEX8oCCXLjU3e10nK1eGePrpELt3\newOdqqpcZsyIcu21UZ2dKeIzBbkc5h//cNi0KcimTQFefTXIq68GaWz0+kmqqxPMmBFl8uQYZ50V\np7zc52JFBFCQlyTXhbo6h61bA2zb5rBtWwBrveDeufPQqQWO4zJiRILzz48zaVKMMWPiBIM+Fi4i\nHeo2yI0xAeA+4FSgBZhtrd2Ssv1C4FYgBjxsrX0wS7VKN1wX9u+HhgaH3bsddu1yaGyEd94pY9cu\n7/F77wXYti3A/v0f/TbyyCMTTJrUyqhRCUaNinPqqXFdI1ykAKTTIr8IiFhrzzTGnAHMBb4MYIwJ\nA3cDY4EDwHpjzHJr7a5sFZwLrtv5kkgcugXvNhyG+npIJBzi8UPb4/HUxTnscWsrtLY6B+/HYt7j\nlhaSi0M0euh+YyM0Nqbeevf37nVoaHCSt14NH3WoDyQScRk2LMGwYQmGD08wbJjL8OEJTjghoVPn\nRQpUOkE+DlgFYK3daIwZk7JtJLDFWlsPYIxZB5wD1GS60PXrg8yaBU1NVYAXlm3cTvKnbX3727b7\nqetdt6/j5fxpulZUuAwY4DJ0aIITT3Q54ggYONBl8GCXIUNcTjyxnH79GhkyxHvOoEFoaKBIkUkn\nyAcADSmP48aYkLU21sG2fcDArt5s0KAKQqGed7Qefzx86lPQ3HwohVIDqbNwalvf/rbtfur69vfb\nL4HA4bdtSzDorWtbgsFD69uWUOjw++Fwx0t5OUQi3m3qUlnZ8RIKOUB3yVyR9n7Operq/Oy3UV09\no7p6Jht1pRPkezm8uRlIhnhH2/oDH3b1ZvX1jT0qsM2RR8Kf/tSfurp9vXp9NlVX576uaNRbuuNH\nbelQXT2junqmGOvq6hdAOlc/XA9MAkj2kW9O2fYmcKIxZrAxpgyvW+WFXlUpIiK9kk6LfCkw3hiz\nAe9v+JnGmOlAlbV2oTHmeuBpvF8KD1tr38teuSIi0l63QW6tTQBz2q1+K2X7k8CTGa5LRETSpIkl\nREQKnIJcRKTAKchFRAqcglxEpMApyEVECpzjdnZ+u4iIFAS1yEVECpyCXESkwCnIRUQKnIJcRKTA\nKchFRAqcglxEpMApyEVEClw6l7H1hTHmX4Fp1trpycdnAPPwJnleba39cbvn9wMWA0PwZir6mrW2\nLku13QRckHx4BHCktfbIds+ZhzdNXttV5L9srU2dTSkbdTnAu8Dfk6tesNb+oN1zrgKuwduPd1hr\nV2SzpuTPHIj3fzMAKAOut9a+0O45Odtf+TqheHIO3IeBT+JNtHqHtXZ5yvbrgNlA23F9jbXW5qi2\nTXgTyQBss9bOTNnm1/66Argi+TACnIb3WfwwuT3n+8sY81ngTmvt540xJwCPAi7wV+Da5NVk257b\n5XHYE3kZ5MkP9QTgtZTVDwAXA1uBWmPM6dbaV1O2fx3YbK293Rjzf4Gbge9koz5r7c+BnydrXQF8\nr4OnjQYmWGt3Z6OGThwPbLLWXtjRRmPMkcC3gTF4B/46Y8waa21Lluu6HnjGWnuPMcYAvwNGtXtO\nLvdXvk4ofhnwgbV2hjFmMN7xvzxl+2jgcmvtX3JQy0HGmAjgWGs/38E23/aXtfZRvKDEGHMv3i+R\n1BnKcrq/jDHfA2bg7QeAu4CbrbXPGWMewDvGlqa8pNPjsKfytWtlA14wA2CMGQCUW2vfsda6eBNZ\nfLHdaw5OEg081cH2jDPGTAXqrbWr260PACcCC40x640xV2a7lqTRwDHGmGeNMSuToZnqM8B6a21L\nsrW7BTglB3XdDSxI3g8Bzakbfdhfh00ojveLrc3BCcWttVGgbULxXKgBbkned/BauKlGAz8wxqwz\nxvyA3DkVqDDGrDbGrE2GThs/9xcAyQnhP2WtXdhuU6731zvA1HY///nk/Y4yqavjsEd8bZEbY2YB\n17VbPdNa+/+MMZ9PWTeAQ3/Wgffn9/B2r0udCLrbSaAzUOPLwA+Ar3TwskrgV3i/kYPAs8aYV6y1\nr2eipi7quhb4T2ttjTFmHF53xtiU7T2eLDtDdc201r6c/ItgMfDddtuzvr/ayeiE4plird0PYIzp\nDyzB+6sy1e+Be/E+C0uNMVNy0TUGNAK/BB7C+4X7lDHG+L2/UvwQ+HEH63O6v6y1fzTGfDJllZNs\neELH+6Wr47BHfA1ya+0iYFEaT01nkufU53Q7CXS6OqvRGPN/gA876dNqBOZZaxuTz12L16rJWDB1\nVJcxpoJkK85au84Yc7QxJvVg6vFk2ZmoK1nbyXgfrButtc+325z1/dVORicUzyRjzHF4f37fZ639\nbcp6B7in7XsDY0wtcDqQiyB/G6/V7QJvG2M+AI4C/hf/99cRgLHWPttuvZ/7q00i5X53mQWHH4c9\nkq9dK4ex1u4FosaY45P/QROAP7d72sFJooGJHWzPtC/i/bnUkRF4fYXBZB/iOGBTlusBuI1ka9cY\ncyrwvykhDvAScLYxJpL8AnIk3pcwWZX8pVcDTLfWdrTPcr2/8nJCcWPMUGA18H1r7cPtNg8A/mqM\nqUp+Bs4HctVXfiVe/y3GmKOTtfwzuc3vCdjPAZ7pYL2f+6vNqyk9Cx1lUlfHYY/k5ZednZgD/Abv\nT+/V1toXAYwxq4EpwP3Afxlj1gFRYHqW6zHAmsNWeBNRb7HWLjfGPAZsBFqB/7bW/i3L9YD3Bexi\nY8xkvJb5FR3UNR/vgAoAP7LWNnf2Zhn0n3hfrs5Ldts3WGu/7OP+ytcJxX8IDAJuMca09ZU/CFQm\n6/oh8CzeCIdnrLUrc1TXIuDR5GfLxQv2S40xfu8v8D6HWw8+OPz/0a/91eYG4MHkL7g38brLMMb8\nN1632UeOw97+IF3GVkSkwBVE14qIiHROQS4iUuAU5CIiBU5BLiJS4BTkIiIFTkEuIlLgFOQiIgXu\n/wMfkhaaq1PcJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11195a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_values = np.linspace(-10, 10)\n",
    "y_values = 1/(1 + np.exp(-1*X_values))\n",
    "plt.plot(X_values, y_values, color='Blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic Regression model is simple, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(C=2)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Our Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          B       0.93      0.97      0.95       105\n",
      "          M       0.95      0.88      0.91        66\n",
      "\n",
      "avg / total       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = log_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Metrics in Classification Report\n",
    "\n",
    "The classification report that we printed out contains the following metrics:\n",
    "\n",
    "- precision - the ratio of correctly predicted positive observations (true positives) to the total predicted positive observations. In the context of this problem, for the M class, this is the number of correctly predicted diagnoses of malignant tumors divided by the total number of predicted malignant tumors. \n",
    "- recall - the ratio of correctly predicted positive observations to all observations actually in that class.\n",
    "- f1-score - a weighted average of precision and recall.\n",
    "\n",
    "Based on the classification report our model is very accurate considering we did not have to do much preprocessing.\n",
    "\n",
    "A more intuitive evaluation metric is accuracy, which is simply the number of correct predictions divided by the total number of predictions. Next we will take a look at this metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logistic regression model has an accuracy of: 93.56725146198829 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Our logistic regression model has an accuracy of: {} %'.format(100*accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do any better by experimenting with the value of C. This process is called hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x112597898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11254eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C_vals = np.linspace(0.5, 10, num=50)\n",
    "accuracies = []\n",
    "\n",
    "for C_val in C_vals:\n",
    "    model = LogisticRegression(C=C_val)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    accuracies.append(100*accuracy_score(y_test, pred))\n",
    "    \n",
    "plt.plot(C_vals, np.array(accuracies), color='Red')\n",
    "plt.xlabel(\"Value of C parameter\")\n",
    "plt.ylabel(\"Accuracy of Model %\")\n",
    "plt.title(\"Value of C vs. Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that increasing the value of C up to 8 increases the cross-validation accuracy of our model, but going any further does not yield any improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This level of accuracy is pretty good, considering we did not have to do anything to complicated. This is the power of scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          B       0.93      0.92      0.93       105\n",
      "          M       0.88      0.89      0.89        66\n",
      "\n",
      "avg / total       0.91      0.91      0.91       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dtree.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our decision tree model has an accuracy of: 91.22807017543859 %\n"
     ]
    }
   ],
   "source": [
    "print('Our decision tree model has an accuracy of: {} %'.format(100*accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision tree model did a little worse than our logistic regression model but this is expected since single decision trees are usually not strong models by themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods - Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a single decision tree is not very strong, combining the predictions of many different decision trees using majority voting results in a very powerful model called a random forest. Next we will experiment with training a random forest model on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=6, n_jobs=-1, n_estimators=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          B       0.96      0.97      0.97       105\n",
      "          M       0.95      0.94      0.95        66\n",
      "\n",
      "avg / total       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = forest.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our random forest model has an accuracy of: 95.90643274853801 %\n"
     ]
    }
   ],
   "source": [
    "print('Our random forest model has an accuracy of: {} %'.format(100*accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our random forest model outperforms both the logistic regression and single decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get better results and where to go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each of these models, we did not have to do much preprocessing and still achieved excellent cross validation results! However, if we want to go a step further, we can try the following techniques:\n",
    "\n",
    "- More ensemble methods - Bagging, Adaboosting, etc.\n",
    "- Feature engineering - do some research and try to extract more features from the data.\n",
    "- Techniques for dimensionality reduction such as PCA, ICA, LDA.\n",
    "- Get more data!\n",
    "- Combine multiple models using majority voting or stacking!\n",
    "\n",
    "Feel free to explore further and check out the scikit-learn website to try out different models and experiment with different datasets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
